{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional Semantics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we'll be using the 500 document Brown corpus included in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is divided up into two independent parts: the first uses PMI for distinguishing good collocations, and the second involves building a vector space model.\n",
    "\n",
    "For the PMI portion, we'll use a function which extracts the information we need for a particular two word collocation, namely counts of each word individually, counts of the collocation, and the total number of word tokens in the corpus, and then calculates the PMI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_PMI_for_collocation_brown(word1,word2):\n",
    "    word1_count = 0\n",
    "    word2_count = 0\n",
    "    both_count = 0\n",
    "    total_count = 0.0 # so that division results in a float\n",
    "    for sent in brown.sents():\n",
    "        sent = [word.lower() for word in sent]\n",
    "        for i in range(len(sent)):\n",
    "            total_count += 1\n",
    "            if sent[i] == word1:\n",
    "                word1_count += 1\n",
    "                if i < len(sent) - 1 and sent[i + 1] == word2:\n",
    "                    both_count += 1\n",
    "            elif sent[i] == word2:\n",
    "                word2_count += 1\n",
    "    return math.log((both_count/total_count)/((word1_count/total_count)*(word2_count/total_count)), 2)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in a typical use case, we probably wouldn't do it this way, since we'd likely want to calculate PMI across many different words, and collecting the statisitcs for this can be done in a single pass across the corpus for all words, and then the PMI calculated in a separate function. Anyway, let's compare the PMI for two phrases, \"hard work\" and \"some work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.237244531670497\n",
      "1.9135320271049516\n"
     ]
    }
   ],
   "source": [
    "print(get_PMI_for_collocation_brown(\"hard\",\"work\"))\n",
    "print(get_PMI_for_collocation_brown(\"some\",\"work\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on PMI, \"hard work\" appears to be a much better collocation than \"some work\", which matches our intuition. Go ahead and try out this out some other collocations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second part of the notebook, let's first create a sparse document-term matrix, using sci-kit learn. We'll then apply tf-idf weighting and SVD to learn word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
      "['Austin', ',', 'Texas', '--', 'Committee', 'approval', ...]\n",
      "['Several', 'defendants', 'in', 'the', 'Summerdale', ...]\n",
      "['Oslo', 'The', 'most', 'positive', 'element', 'to', ...]\n",
      "['East', 'Providence', 'should', 'organize', 'its', ...]\n",
      "['Plainfield', '--', 'James', 'P.', 'Mitchell', 'and', ...]\n",
      "['Resentment', 'welled', 'up', 'yesterday', 'among', ...]\n",
      "['Appointment', 'of', 'William', 'S.', 'Pfaff', 'Jr.', ...]\n",
      "['City', 'Controller', 'Alexander', 'Hemphill', ...]\n",
      "['Vincent', 'G.', 'Ierulli', 'has', 'been', ...]\n",
      "['Miami', ',', 'Fla.', ',', 'March', '17', '--', 'The', ...]\n",
      "['Austin', ',', 'Texas', '--', 'A', 'Texas', ...]\n",
      "['Rookie', 'Ron', 'Nischwitz', 'continued', 'his', ...]\n",
      "['Philadelphia', ',', 'Jan.', '23', '--', 'Nick', ...]\n",
      "['If', 'the', 'Cardinals', 'heed', 'Manager', 'Gene', ...]\n",
      "['Romantic', 'news', 'concerns', 'Mrs.', 'Joan', ...]\n",
      "['After', 'being', 'closed', 'for', 'seven', 'months', ...]\n",
      "['``', 'A', 'Night', 'in', 'New', 'Orleans', \"''\", ...]\n",
      "['The', 'Baltimore', 'and', 'Ohio', 'Railroad', ...]\n",
      "['London', ',', 'Feb.', '9', '--', 'Vital', 'secrets', ...]\n",
      "['St.', 'Johns', ',', 'Mich.', ',', 'April', '19', '.', ...]\n",
      "['Emory', \"University's\", 'Board', 'of', 'Trustees', ...]\n",
      "['Salem', '(', 'special', ')', '--', 'For', 'a', ...]\n",
      "['About', '70', 'North', 'Providence', 'taxpayers', ...]\n",
      "['Asilomar', ',', 'March', '26', 'Vast', 'spraying', ...]\n",
      "['Probably', 'the', 'hottest', 'thing', 'that', 'has', ...]\n",
      "['Santa', 'Barbara', '--', '``', 'The', 'present', ...]\n",
      "['Elburn', ',', 'Ill.', '--', 'Farm', 'machinery', ...]\n",
      "['Greer', 'Garson', ',', 'world-famous', 'star', 'of', ...]\n",
      "['A', 'cookie', 'with', 'caramel', 'filling', 'and', ...]\n",
      "['Hotel', \"Escape's\", 'Bonanza', 'room', 'has', 'a', ...]\n",
      "['Enrique', 'Jorda', ',', 'conductor', 'and', ...]\n",
      "['At', 'last', 'the', 'White', 'House', 'is', 'going', ...]\n",
      "['For', 'crucial', 'encounter', 'One', 'of', 'the', ...]\n",
      "['There', 'are', ',', 'so', 'my', 'biologist', ...]\n",
      "['Sen.', 'John', 'L.', 'McClellan', 'of', 'Arkansas', ...]\n",
      "['The', 'nation', 'the', 'three-front', 'war', 'At', ...]\n",
      "['The', 'Masters', 'golf', 'tournament', 'proved', ...]\n",
      "['When', 'Mickey', 'Charles', 'Mantle', ',', 'the', ...]\n",
      "['Into', 'Washington', 'on', 'President-elect', 'John', ...]\n",
      "['A', 'philosopher', 'may', 'point', 'out', 'that', ...]\n",
      "['The', 'presidency', ':', 'talking', 'and', ...]\n",
      "['Holders', 'of', 'toll-road', 'bonds', 'are', ...]\n",
      "['Every', 'library', 'borrower', ',', 'or', 'at', ...]\n",
      "['Assembly', 'session', 'brought', 'much', 'good', ...]\n",
      "['Must', 'Berlin', 'remain', 'divided', '?', '?', ...]\n",
      "['A', 'good', 'man', 'departs', '.', 'Goodby', ',', ...]\n",
      "['A', 'shock', 'wave', 'from', 'Africa', 'Word', 'of', ...]\n",
      "['Help', 'when', 'needed', 'If', 'the', 'Dominican', ...]\n",
      "['The', 'study', 'of', 'the', 'St.', 'Louis', \"area's\", ...]\n",
      "['The', \"U.N.'s\", \"'\", 'gravest', 'crisis', \"'\", ...]\n",
      "['Old', ',', 'tired', ',', 'trembling', 'the', 'woman', ...]\n",
      "['Mr.', 'Podger', 'always', 'particularly', 'enjoyed', ...]\n",
      "['Miami', ',', 'Fla.', ',', 'March', '17', '.', 'An', ...]\n",
      "['California', 'Democrats', 'this', 'weekend', 'will', ...]\n",
      "['Sing', \"Sing's\", 'prisoner', 'strike', 'was', ...]\n",
      "['Sizzling', 'temperatures', 'and', 'hot', 'summer', ...]\n",
      "['This', 'is', 'the', 'period', 'during', 'the', ...]\n",
      "['The', 'Providence', 'Journal', 'editorial', '(', ...]\n",
      "['``', 'A', 'lousy', 'job', \"''\", 'Chicago', ',', ...]\n",
      "['``', 'Workers', 'of', 'the', 'party', \"''\", 'to', ...]\n",
      "['For', 'a', 'neutral', 'Germany', 'Soviets', 'said', ...]\n",
      "['Ghost', 'town', '?', '?', 'To', 'the', 'editor', ...]\n",
      "['Escalation', 'unto', 'death', 'The', 'nuclear', ...]\n",
      "['All', 'false', 'gods', 'resemble', 'Moloch', ',', ...]\n",
      "['Resuming', 'atmospheric', 'tests', 'One', 'of', ...]\n",
      "['Everywhere', 'I', 'went', 'in', 'Formosa', 'I', ...]\n",
      "['Broadway', 'the', 'unoriginals', 'To', 'write', 'a', ...]\n",
      "['The', 'most', 'surprising', 'thing', 'about', 'the', ...]\n",
      "['Tenure', 'as', 'criterion', 'I', 'would', 'like', ...]\n",
      "['Confrontation', 'It', 'seems', 'to', 'me', 'that', ...]\n",
      "['It', 'is', 'not', 'news', 'that', 'Nathan', ...]\n",
      "['Television', 'has', 'yet', 'to', 'work', 'out', 'a', ...]\n",
      "['Francois', \"D'Albert\", ',', 'Hungarian-born', ...]\n",
      "['The', 'Theatre-by-the-Sea', ',', 'Matunuck', ',', ...]\n",
      "['The', 'superb', 'intellectual', 'and', 'spiritual', ...]\n",
      "['George', \"Kennan's\", 'account', 'of', 'relations', ...]\n",
      "['Some', 'of', 'the', 'New', 'York', 'Philharmonic', ...]\n",
      "['Had', 'a', 'funny', 'experience', 'at', 'Newport', ...]\n",
      "['Murray', 'Louis', 'and', 'his', 'dance', 'company', ...]\n",
      "['Ring', 'Of', 'Bright', 'Water', ',', 'by', 'Gavin', ...]\n",
      "['Mischa', 'Elman', 'shared', 'last', \"night's\", ...]\n",
      "['Radio', 'is', 'easily', 'outdistancing', ...]\n",
      "['A', 'tribe', 'in', 'ancient', 'India', 'believed', ...]\n",
      "['Elisabeth', 'Schwarzkopf', 'sang', 'so', ...]\n",
      "['As', 'autumn', 'starts', 'its', 'annual', 'sweep', ...]\n",
      "['A', 'year', 'ago', 'it', 'was', 'bruited', 'that', ...]\n",
      "['The', 'reading', 'public', ',', 'the', ...]\n",
      "['As', 'a', 'result', ',', 'although', 'we', 'still', ...]\n",
      "['If', 'the', 'content', 'of', 'faith', 'is', 'to', ...]\n",
      "['One', 'hundred', 'years', 'ago', 'there', 'existed', ...]\n",
      "['The', 'death', 'of', 'a', 'man', 'is', 'unique', ',', ...]\n",
      "['Furthermore', ',', 'as', 'an', 'encouragement', 'to', ...]\n",
      "['I', 'have', ',', 'within', 'the', 'past', 'fifty', ...]\n",
      "['``', 'The', 'Lord', 'is', 'my', 'light', 'and', 'my', ...]\n",
      "['But', ',', 'again', ',', 'we', 'have', 'no', 'real', ...]\n",
      "['Few', 'persons', 'who', 'join', 'the', 'Church', ...]\n",
      "['Men', 'need', 'unity', 'and', 'they', 'need', 'God', ...]\n",
      "['When', 'they', 'say', 'that', 'under', 'no', ...]\n",
      "['Much', 'more', 'than', 'shelter', ',', 'housing', ...]\n",
      "['If', 'we', 'look', 'about', 'the', 'world', 'today', ...]\n",
      "['To', 'what', 'extent', 'and', 'in', 'what', 'ways', ...]\n",
      "['Individuals', 'possessing', 'unusual', 'gifts', ...]\n",
      "['Pope', 'Leo', '13', ',', 'on', 'the', '13th', 'day', ...]\n",
      "['I', 'am', 'a', 'magazine', ';', ';', 'my', 'name', ...]\n",
      "['Too', 'often', 'a', 'beginning', 'bodybuilder', ...]\n",
      "['The', 'most', 'beautiful', 'bed', 'of', 'pansies', ...]\n",
      "['Five', ',', 'four', ',', 'three', ',', 'two', ',', ...]\n",
      "['The', 'lyric', 'beauties', 'of', \"Schubert's\", ...]\n",
      "['At', 'the', 'Westminster', 'KC', 'Dog', 'Show', 'in', ...]\n",
      "['At', 'one', 'time', ',', 'to', 'most', 'Americans', ...]\n",
      "['The', 'design', 'of', 'a', 'mechanical', ...]\n",
      "['Scientists', 'say', 'that', 'the', 'world', 'and', ...]\n",
      "['Orlando', ',', 'Fla.', ',', 'Feb.', '2', '--', 'The', ...]\n",
      "['The', 'average', 'reader', 'of', 'this', 'magazine', ...]\n",
      "['Livery', 'stable', '--', 'J.', 'Vernon', ',', ...]\n",
      "['Those', 'who', 'have', 'never', 'traveled', 'the', ...]\n",
      "['Built', 'upon', 'seven', 'hills', ',', 'Istanbul', ...]\n",
      "['(', 'Do', 'start', 'fires', 'one', 'or', 'two', ...]\n",
      "['Note', ':', 'Directions', 'are', 'written', 'for', ...]\n",
      "['Hotei', 'is', '23', 'feet', 'long', 'with', 'an', ...]\n",
      "['You', 'can', 'build', 'this', 'vacation', 'cottage', ...]\n",
      "['The', 'old-time', 'bridges', 'over', 'the', ...]\n",
      "['It', 'is', 'a', 'good', 'eight', 'years', 'now', ...]\n",
      "['Draw', 'a', 'line', 'across', 'the', 'country', 'at', ...]\n",
      "['Most', 'recreation', 'work', 'calls', 'for', 'a', ...]\n",
      "['Your', 'invitation', 'to', 'write', 'about', 'Serge', ...]\n",
      "['Roy', 'Mason', 'is', 'essentially', 'a', 'landscape', ...]\n",
      "['The', 'Russian', 'gymnasts', 'beat', 'the', 'tar', ...]\n",
      "['Every', 'taxpayer', 'is', 'well', 'aware', 'of', ...]\n",
      "[\"Oersted's\", 'boyhood', 'represented', 'a', 'minimal', ...]\n",
      "['Knowing', 'specifically', 'what', 'the', 'many', ...]\n",
      "['Marketing', 'in', 'the', 'new', 'decade', 'will', ...]\n",
      "['The', 'controversy', 'of', 'the', 'last', 'few', ...]\n",
      "['General', 'How', 'long', 'has', 'it', 'been', ...]\n",
      "['The', 'long', 'and', 'ever-increasing', 'column', ...]\n",
      "['Throughout', 'history', ',', 'the', 'man', 'who', ...]\n",
      "['Sixty', 'miles', 'north', 'of', 'New', 'York', ...]\n",
      "['In', 'the', 'period', 'since', 'the', 'end', 'of', ...]\n",
      "['New', 'rule', 'no.', '2', ':', ':', \"Don't\", 'build', ...]\n",
      "['There', 'comes', 'a', 'time', 'in', 'the', 'lives', ...]\n",
      "['In', 'American', 'romance', ',', 'almost', 'nothing', ...]\n",
      "['She', 'was', 'just', 'another', 'freighter', 'from', ...]\n",
      "['There', 'is', 'a', 'pause', 'in', 'the', 'merriment', ...]\n",
      "['``', 'The', 'food', 'is', 'wonderful', 'and', 'it', ...]\n",
      "['Buffeted', 'by', 'swirling', 'winds', ',', 'the', ...]\n",
      "['Are', 'you', 'retiring', 'now', '?', '?', 'If', 'so', ...]\n",
      "['Some', 'recent', 'writings', 'assume', 'that', 'the', ...]\n",
      "['In', 'tradition', 'and', 'in', 'poetry', ',', 'the', ...]\n",
      "['She', 'gave', 'herself', 'a', 'title', ',', 'Lady', ...]\n",
      "['Franklin', 'D.', 'Lee', 'proved', 'a', 'man', 'of', ...]\n",
      "['Ten-year-old', 'Richard', 'Stewart', 'had', 'been', ...]\n",
      "['From', 'time', 'to', 'time', 'the', 'medium', ...]\n",
      "['Farming', 'is', 'confining', '.', 'The', \"farmer's\", ...]\n",
      "['Everyone', 'with', 'a', 'personal', 'or', 'group', ...]\n",
      "['In', 'Poughkeepsie', ',', 'N.Y.', ',', ',', 'in', ...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On', 'April', '17', ',', '1610', ',', 'the', ...]\n",
      "['Thomas', 'Douglas', ',', 'fifth', 'Earl', 'of', ...]\n",
      "['The', 'letters', 'of', 'the', 'common', 'soldiers', ...]\n",
      "['The', 'popularity', 'of', 'folklore', 'in', ...]\n",
      "['As', 'part', 'of', 'the', 'same', 'arrangement', ',', ...]\n",
      "['A', 'tsunami', 'may', 'be', 'started', 'by', 'a', ...]\n",
      "['In', \"Ireland's\", 'County', 'Limerick', ',', 'near', ...]\n",
      "['I', 'do', 'not', 'mean', 'to', 'suggest', 'that', ...]\n",
      "['The', 'first', 'rattle', 'of', 'the', 'machine', ...]\n",
      "['My', 'interviews', 'with', 'teen-agers', 'confirmed', ...]\n",
      "['This', 'should', 'be', 'used', 'frequently', '(', ...]\n",
      "['Except', 'for', 'the', 'wine', 'waiter', 'in', 'a', ...]\n",
      "['Giffen', 'replied', 'punctually', 'and', ...]\n",
      "['But', 'certainly', 'the', 'New', 'Frontier', 'has', ...]\n",
      "['Surrounded', 'by', 'ancient', 'elms', ',', 'the', ...]\n",
      "['The', 'person', 'who', 'left', 'the', 'buggy', ...]\n",
      "['Her', 'father', ',', 'James', 'Upton', ',', 'was', ...]\n",
      "['In', 'a', 'few', 'school', 'districts', 'one', ...]\n",
      "['The', 'soybean', 'seed', 'is', 'the', 'most', ...]\n",
      "['To', 'hold', 'a', 'herd', 'of', 'cattle', 'on', 'a', ...]\n",
      "['It', 'was', 'John', 'who', 'found', 'the', 'lion', ...]\n",
      "['The', 'missionary', 'obligation', 'to', 'proclaim', ...]\n",
      "['Yet', 'a', 'crowd', 'came', 'out', 'to', 'see', ...]\n",
      "['This', 'conclusion', 'is', 'dependent', 'on', 'the', ...]\n",
      "['In', 'the', 'final', 'accounting', ',', 'these', ...]\n",
      "['A', 'little', 'farther', 'along', 'the', 'road', ...]\n",
      "['Rare', ',', 'indeed', ',', 'is', 'the', 'Harlem', ...]\n",
      "['Color', 'was', 'delayed', 'until', '1935', ',', ...]\n",
      "['In', 'general', ',', 'religious', 'interest', ...]\n",
      "['With', 'capital', 'largely', 'squandered', ',', ...]\n",
      "['They', 'would', 'not', 'be', 'pleased', 'to', 'have', ...]\n",
      "['The', 'deep', 'water', 'is', 'used', 'by', 'many', ...]\n",
      "['The', 'achievement', 'of', 'the', 'desegregation', ...]\n",
      "['Northern', 'liberals', 'are', 'the', 'chief', ...]\n",
      "['In', 'the', 'past', ',', 'the', 'duties', 'of', ...]\n",
      "['Can', 'thermonuclear', 'war', 'be', 'set', 'off', ...]\n",
      "['Suddenly', ',', 'however', ',', 'their', 'posture', ...]\n",
      "['It', 'was', 'a', 'fortunate', 'time', 'in', 'which', ...]\n",
      "['Die', 'Frist', 'ist', 'um', ',', 'und', 'wiederum', ...]\n",
      "['Once', 'again', ',', 'as', 'in', 'the', 'days', 'of', ...]\n",
      "['Tobacco', 'Road', 'is', 'dead', '.', 'Long', 'live', ...]\n",
      "['Another', 'element', 'to', 'concern', 'the', ...]\n",
      "['The', 'North', 'and', 'the', 'South', 'were', 'in', ...]\n",
      "['As', 'cells', 'coalesced', 'into', 'organisms', ',', ...]\n",
      "['Nothing', 'like', 'Godot', ',', 'he', 'arrived', ...]\n",
      "['Even', 'Hemingway', ',', 'for', 'all', 'his', ...]\n",
      "['There', 'were', 'fences', 'in', 'the', 'old', 'days', ...]\n",
      "['It', 'is', 'worth', 'dwelling', 'in', 'some', ...]\n",
      "['The', '``', 'reality', \"''\", 'to', 'which', 'they', ...]\n",
      "['After', 'only', 'eighteen', 'years', 'of', ...]\n",
      "['Among', 'the', 'recipients', 'of', 'the', 'Nobel', ...]\n",
      "['Today', 'the', 'private', 'detective', 'will', ...]\n",
      "['It', 'will', 'be', 'shown', 'that', 'the', ...]\n",
      "['American', 'democratic', 'thought', ',', 'pointed', ...]\n",
      "['Does', 'our', 'society', 'have', 'a', 'runaway', ',', ...]\n",
      "['During', 'the', 'last', 'years', 'of', 'Woodrow', ...]\n",
      "['The', \"President's\", 'personality', 'would', 'have', ...]\n",
      "['For', 'this', 'change', 'is', 'not', 'a', 'change', ...]\n",
      "['All', 'of', 'which', 'brings', 'up', 'another', ...]\n",
      "['In', 'recollection', 'he', 'has', 'said', ':', '``', ...]\n",
      "['Important', 'as', 'was', 'Mr.', \"O'Donnell's\", ...]\n",
      "['Some', 'years', 'ago', 'Julian', 'Huxley', ...]\n",
      "['Copernicus', 'did', 'not', 'question', 'it', ',', ...]\n",
      "[\"Henrietta's\", 'feeling', 'of', 'identity', 'with', ...]\n",
      "['This', 'time', 'he', 'was', 'making', 'no', ...]\n",
      "['More', 'likely', ',', 'you', 'simply', 'told', ...]\n",
      "['With', 'each', 'song', 'he', 'gave', 'verbal', ...]\n",
      "['The', 'United', 'States', 'is', 'always', 'ready', ...]\n",
      "['The', 'group', ',', 'upon', 'the', 'issuance', 'of', ...]\n",
      "['Fortunately', 'the', 'hole', 'was', 'found', 'at', ...]\n",
      "['She', 'describes', ',', 'first', ',', 'the', ...]\n",
      "['When', 'Harold', 'Arlen', 'returned', 'to', ...]\n",
      "['I', 'feel', 'obliged', 'to', 'describe', 'this', ...]\n",
      "['If', 'she', 'were', 'not', 'at', 'home', ',', 'Mama', ...]\n",
      "['Impressive', 'as', 'this', 'enumeration', 'is', ',', ...]\n",
      "['Two', 'facets', 'of', 'this', 'aspect', 'of', 'the', ...]\n",
      "['The', 'late', 'R.', 'G.', 'Collingwood', ',', 'a', ...]\n",
      "['``', 'Suppose', 'you', 'take', 'Mr.', \"Hearst's\", ...]\n",
      "[\"Trevelyan's\", 'Liberalism', 'was', 'above', 'all', ...]\n",
      "['I', 'stood', 'on', 'a', 'table', ',', 'surrounded', ...]\n",
      "['He', 'remembered', 'every', 'detail', 'of', 'his', ...]\n",
      "['In', 'one', 'of', 'the', 'very', 'few', 'letters', ...]\n",
      "['As', 'he', 'had', 'done', 'on', 'his', 'first', ...]\n",
      "['Mando', ',', 'pleading', 'her', 'cause', ',', 'must', ...]\n",
      "['``', 'Let', 'him', 'become', 'honest', ',', 'and', ...]\n",
      "['Samuel', 'Gorton', ',', 'founder', 'of', 'Warwick', ...]\n",
      "['When', 'Fred', 'wheeled', 'him', 'back', 'into', ...]\n",
      "['It', 'was', 'not', 'until', 'we', 'had', 'returned', ...]\n",
      "['Criticism', 'is', 'as', 'old', 'as', 'literary', ...]\n",
      "['But', 'it', 'would', 'not', 'be', 'very', ...]\n",
      "['From', 'New', 'Jersey', ',', 'Morgan', 'hastened', ...]\n",
      "['For', 'by', 'now', 'the', 'original', 'cause', 'of', ...]\n",
      "['It', 'usually', 'turned', 'out', 'well', 'for', ...]\n",
      "['The', 'turn', 'of', 'the', 'century', ',', 'or', ...]\n",
      "['To', 'do', 'so', ',', 'something', 'was', ...]\n",
      "['In', 'the', 'imagination', 'of', 'the', 'nineteenth', ...]\n",
      "['The', 'Bishop', 'of', 'Gloucester', 'described', ...]\n",
      "['The', 'fall', 'of', 'Rome', ',', 'the', 'discovery', ...]\n",
      "['Stephens', 'had', 'written', 'his', 'classic', '``', ...]\n",
      "['He', 'looked', 'at', 'her', 'as', 'she', 'spoke', ...]\n",
      "['From', '1613', 'on', ',', 'if', 'the', 'lists', ...]\n",
      "['Writers', 'of', 'this', 'class', 'of', 'science', ...]\n",
      "['We', 'often', 'say', 'of', 'a', 'person', 'that', ...]\n",
      "['For', 'here', 'if', 'anywhere', 'in', 'contemporary', ...]\n",
      "['In', 'a', 'pessimistic', 'assessment', 'of', 'the', ...]\n",
      "['The', 'recent', 'experiments', 'in', 'the', 'new', ...]\n",
      "['Let', 'us', 'see', 'just', 'how', 'typical', 'Krim', ...]\n",
      "['There', 'had', 'been', 'signs', 'and', 'portents', ...]\n",
      "['The', 'Office', 'of', 'Business', 'Economics', '(', ...]\n",
      "['In', 'most', 'of', 'the', 'less', 'developed', ...]\n",
      "['You', 'have', 'heard', 'him', 'tell', 'these', ...]\n",
      "['Origin', 'of', 'state', 'automobile', 'practices', ...]\n",
      "['The', 'Rhode', 'Island', 'property', 'tax', 'There', ...]\n",
      "['Local', \"industry's\", 'investment', 'in', 'Rhode', ...]\n",
      "['Special', 'districts', 'in', 'Rhode', 'island', '.', ...]\n",
      "['Rhode', 'Island', 'Heritage', 'Week', 'proclamation', ...]\n",
      "['Be', 'it', 'enacted', 'by', 'the', 'Senate', 'and', ...]\n",
      "['In', 'the', 'same', 'period', ',', '431', ...]\n",
      "['Another', 'recent', 'achievement', 'was', 'the', ...]\n",
      "['(', 'E', ')', 'In', 'addition', 'to', 'the', ...]\n",
      "['Mr.', 'Dooley', '.', 'Mr.', 'Speaker', ',', 'for', ...]\n",
      "['From', 'its', 'inception', 'in', '1920', 'with', ...]\n",
      "['At', 'the', 'entrance', 'side', 'of', 'the', ...]\n",
      "['A', 'former', 'Du', 'Pont', 'official', 'became', ...]\n",
      "['It', 'is', 'not', 'a', 'medieval', 'mental', 'quirk', ...]\n",
      "['Foreign', 'policy', 'in', 'its', 'total', 'context', ...]\n",
      "['While', 'there', 'should', 'be', 'no', 'general', ...]\n",
      "['Wildlife', 'habitat', 'resources', 'In', '1960', ...]\n",
      "['Strategy', 'and', 'tactics', 'of', 'the', 'U.S.', ...]\n",
      "['Purchase', 'authorizations', 'will', 'include', ...]\n",
      "['This', 'broad', 'delegation', 'leaves', 'within', ...]\n",
      "['If', 'you', 'elect', 'to', 'use', 'the', 'Standard', ...]\n",
      "['The', 'one-', 'or', 'two-season', 'hunt', ',', 'of', ...]\n",
      "['When', 'the', 'Brown', '&', 'Sharpe', ...]\n",
      "['Sales', 'and', 'net', 'income', 'for', 'the', 'year', ...]\n",
      "['Following', 'the', 'term', 'of', 'service', 'in', ...]\n",
      "['In', 'recent', 'months', ',', 'much', 'attention', ...]\n",
      "['Between', 'meetings', 'he', 'helps', 'the', ...]\n",
      "['1', '.', 'Introduction', 'It', 'has', 'recently', ...]\n",
      "['Abstract', 'Experiments', 'were', 'made', 'on', 'an', ...]\n",
      "['A', 'band', 'viscometer', 'is', 'shown', 'in', ...]\n",
      "['A', 'proton', 'magnetic', 'resonance', 'study', 'of', ...]\n",
      "['Polyphosphates', 'gave', 'renewed', 'life', 'to', ...]\n",
      "['The', 'thermal', 'exchange', 'of', 'chlorine', ...]\n",
      "['The', 'Poynting-Robertson', 'effect', '(', ...]\n",
      "['Biological', 'warfare', 'Biological', 'warfare', ...]\n",
      "['A', 'variety', 'of', 'techniques', 'have', 'been', ...]\n",
      "['Apart', 'from', 'the', 'honeybee', ',', ...]\n",
      "['Many', 'other', '(', 'probably', 'nearly', 'all', ...]\n",
      "['The', 'bronchus', 'and', 'pulmonary', 'artery', 'in', ...]\n",
      "['Some', 'of', 'the', 'features', 'of', 'the', 'top', ...]\n",
      "['Interestingly', 'enough', ',', 'the', 'effect', 'of', ...]\n",
      "['Introduction', 'Muscle', 'weakness', 'is', 'now', ...]\n",
      "['Purification', 'of', 'the', 'conjugates', 'In', ...]\n",
      "['Since', 'emotional', 'reactions', 'in', 'the', ...]\n",
      "['6.4', '.', 'The', 'primary', 'decomposition', ...]\n",
      "['7-1', '.', 'Examples', 'of', 'binomial', ...]\n",
      "['Consider', 'a', 'simple', ',', 'closed', ',', ...]\n",
      "['Introduction', '.', 'In', '1', 'we', 'investigate', ...]\n",
      "['These', 'societies', 'can', 'expect', 'to', 'face', ...]\n",
      "['2', ':', ':', 'Some', 'of', 'the', 'major', ...]\n",
      "['Overwhelmed', 'with', 'the', 'care', 'of', 'five', ...]\n",
      "['Sentiment', '.', 'Tension', 'management', 'and', ...]\n",
      "['A', 'royal', 'decree', 'issued', 'in', '1910', ',', ...]\n",
      "['With', 'this', 'evidence', 'in', 'mind', ',', 'the', ...]\n",
      "['Thus', 'it', 'is', 'reasonable', 'to', 'believe', ...]\n",
      "['Control', 'of', 'socioeconomic', 'status', 'It', ...]\n",
      "['Analysis', 'Analysis', 'means', 'the', 'evaluation', ...]\n",
      "['It', 'is', 'not', 'easy', 'for', 'the', 'therapist', ...]\n",
      "['The', 'many', 'linguistic', 'techniques', 'for', ...]\n",
      "['In', 'sentences', ',', 'patterns', 'of', 'stress', ...]\n",
      "['It', 'is', 'obvious', 'enough', 'that', 'linguists', ...]\n",
      "['There', 'are', 'more', 'stems', 'per', 'item', 'in', ...]\n",
      "['Doubtless', 'it', 'was', 'inevitable', 'that', ...]\n",
      "['An', 'analysis', 'of', 'the', 'election', 'falls', ...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unemployed', 'older', 'workers', 'who', 'have', 'no', ...]\n",
      "['But', 'briefly', ',', 'the', 'topping', ...]\n",
      "['In', 'assessing', 'the', 'outlook', 'for', ...]\n",
      "['Wage-price', 'policies', 'of', 'industry', 'are', ...]\n",
      "['In', 'the', 'century', 'from', '1815', 'to', '1914', ...]\n",
      "['Mr.', 'Justice', 'Black', 'was', 'one', 'of', 'the', ...]\n",
      "['How', 'else', 'can', 'one', 'explain', ',', 'for', ...]\n",
      "['A', '.', 'Reasons', 'for', 'selecting', 'mail', ...]\n",
      "['The', 'vast', 'Central', 'Valley', 'of', ...]\n",
      "['9', '.', 'Martin', 'and', 'Stendler', 'present', ...]\n",
      "['The', 'Summary', 'Report', 'On', 'Desegregation', ...]\n",
      "['The', 'next', 'question', 'is', 'whether', 'board', ...]\n",
      "['Unfortunately', ',', 'however', ',', 'and', 'for', ...]\n",
      "['The', 'preconditions', 'of', 'sociology', 'have', ...]\n",
      "['But', 'neither', 'was', 'the', 'statement', ...]\n",
      "['The', 'injured', 'German', 'veteran', 'was', 'a', ...]\n",
      "['Whenever', 'artists', ',', 'indeed', ',', 'turned', ...]\n",
      "['Cook', 'had', 'discovered', 'a', 'beef', 'in', 'his', ...]\n",
      "['The', 'plant', 'was', 'located', 'west', 'of', 'the', ...]\n",
      "['Some', 'who', 'have', 'written', 'on', 'Utopia', ...]\n",
      "['During', 'the', 'Dorr', 'trial', 'the', 'Democratic', ...]\n",
      "['Rather', 'than', 'being', 'deceived', ',', 'the', ...]\n",
      "['Los', 'Angeles', 'in', '1957', 'finally', 'bowed', ...]\n",
      "['Before', 'losing', 'itself', 'in', 'the', 'sands', ...]\n",
      "['Those', 'whom', 'I', 'wish', 'to', 'address', 'with', ...]\n",
      "['The', 'Sane', 'Society', 'is', 'an', 'ambitious', ...]\n",
      "['If', 'one', 'characteristic', 'distinguishes', ...]\n",
      "['So', 'far', 'these', 'remarks', ',', 'like', 'most', ...]\n",
      "['Critically', 'invisible', ',', 'modern', 'revolt', ...]\n",
      "['Anglo-Saxon', 'and', 'Greek', 'epic', 'each', ...]\n",
      "['Recent', 'criticism', 'of', 'Great', 'Expectations', ...]\n",
      "['The', 'following', 'items', 'may', 'be', 'specified', ...]\n",
      "['In', 'the', 'Midwest', ',', 'oxidation', 'ponds', ...]\n",
      "['Thus', ',', 'the', 'three', 'main', 'categories', ...]\n",
      "['Two', 'metabolites', '(', '1', ',', 'and', '2', ')', ...]\n",
      "['Organization', ':', 'In', 'this', 'publication', ...]\n",
      "['Because', 'individual', 'classes', 'of', 'foods', ...]\n",
      "['Furthermore', ',', 'it', 'has', 'made', 'an', ...]\n",
      "['Within', 'only', 'a', 'few', 'years', ',', 'foamed', ...]\n",
      "['Temperature', 'of', 'the', 'wash', 'and', 'rinse', ...]\n",
      "['High-gain', ',', 'photoelectronic', 'image', ...]\n",
      "['The', 'set', 'of', 'all', 'decisions', 'is', ...]\n",
      "['A', 'gyro-stabilized', 'platform', 'system', ',', ...]\n",
      "['Thirty-three', 'Scotty', 'did', 'not', 'go', 'back', ...]\n",
      "['Where', 'their', 'sharp', 'edges', 'seemed', ...]\n",
      "['Mickie', 'sat', 'over', 'his', 'second', ...]\n",
      "['The', 'Bishop', 'looked', 'at', 'him', 'coldly', ...]\n",
      "['Payne', 'dismounted', 'in', 'Madison', 'Place', ...]\n",
      "['With', 'a', 'sneer', ',', 'the', 'man', 'spread', ...]\n",
      "['If', 'the', 'crummy', 'bastard', 'could', 'write', ...]\n",
      "['Rousseau', 'is', 'so', 'persuasive', 'that', ...]\n",
      "['It', 'was', 'the', 'first', 'time', 'any', 'of', ...]\n",
      "['That', 'summer', 'the', 'gambling', 'houses', 'were', ...]\n",
      "['Standing', 'in', 'the', 'shelter', 'of', 'the', ...]\n",
      "['She', 'was', 'a', 'child', 'too', 'much', 'a', ...]\n",
      "['In', 'the', 'dim', 'underwater', 'light', 'they', ...]\n",
      "['He', 'brought', 'with', 'him', 'a', 'mixture', 'of', ...]\n",
      "['Beth', 'was', 'very', 'still', 'and', 'her', ...]\n",
      "['The', 'red', 'glow', 'from', 'the', 'cove', 'had', ...]\n",
      "['Burly', 'leathered', 'men', 'and', 'wrinkled', ...]\n",
      "['She', 'was', 'getting', 'real', 'dramatic', '.', ...]\n",
      "['There', 'was', 'one', 'fact', 'which', 'Rector', ...]\n",
      "['She', 'concluded', 'by', 'asking', 'him', 'to', ...]\n",
      "['Beckworth', 'handed', 'the', 'pass', 'to', 'the', ...]\n",
      "['I', 'would', 'not', 'want', 'to', 'be', 'one', 'of', ...]\n",
      "['It', 'was', 'not', 'as', 'though', 'she', 'noted', ...]\n",
      "['His', 'eyes', 'were', 'old', 'and', 'they', 'never', ...]\n",
      "['He', 'was', 'in', 'his', 'mid-fifties', 'at', 'this', ...]\n",
      "['But', 'they', 'all', 'said', ',', '``', 'No', ',', ...]\n",
      "['``', 'But', 'tell', 'me', ',', 'doctor', ',', ...]\n",
      "['Going', 'downstairs', 'with', 'the', 'tray', ',', ...]\n",
      "['Was', 'it', 'love', '?', '?', 'I', 'had', 'no', ...]\n",
      "['There', 'were', 'thirty-eight', 'patients', 'on', ...]\n",
      "['I', 'was', 'giving', 'the', 'parked', 'cars', 'the', ...]\n",
      "['The', 'fat', 'man', 'said', ',', '``', 'All', 'we', ...]\n",
      "['His', 'jowls', 'were', 'spiked', 'by', 'barbs', 'of', ...]\n",
      "['Then', 'he', 'turned', 'the', 'telephone', 'over', ...]\n",
      "['Eight', ',', 'nine', 'steps', 'above', 'him', ',', ...]\n",
      "['He', 'put', 'in', 'a', 'call', 'to', 'Cunningham', ...]\n",
      "['``', 'Well', \"''\", '--', 'said', 'Mr.', 'Skyros', ...]\n",
      "[\"Maude's\", 'long', 'nose', 'unexpectedly', 'wrinkled', ...]\n",
      "['``', 'Not', 'since', 'last', 'night', '.', 'I', ...]\n",
      "['``', 'Right', \"''\", ',', 'said', 'the', ...]\n",
      "['Andy', 'did', 'not', 'see', 'the', 'newspapers', ...]\n",
      "['His', 'son', 'watched', 'until', 'he', 'got', 'as', ...]\n",
      "['A', 'man', 'with', 'a', 'sketch', 'pad', 'in', ...]\n",
      "['``', 'Through', 'a', 'door', 'conveniently', ...]\n",
      "['``', 'Dammit', ',', 'Phil', ',', 'are', 'you', ...]\n",
      "['The', 'safe', 'at', 'Ingleside', 'District', ...]\n",
      "['``', 'She', 'says', 'she', 'has', 'to', 'finish', ...]\n",
      "['Slowly', 'he', 'pulled', 'out', 'the', 'hand', ...]\n",
      "['Harbor', 'Point', 'sticks', 'out', 'into', 'the', ...]\n",
      "['But', 'the', 'police', 'have', 'dropped', 'the', ...]\n",
      "['In', 'good', 'time', 'I', 'shall', 'get', 'to', ...]\n",
      "[\"That's\", 'what', 'the', 'man', 'had', 'said', '.', ...]\n",
      "['About', 'halfway', 'back', 'Pops', 'groped', ...]\n",
      "['Now', 'that', 'he', 'knew', 'himself', 'to', 'be', ...]\n",
      "['The', 'expense', 'and', 'time', 'involved', 'are', ...]\n",
      "['This', 'was', 'not', ',', 'for', 'the', 'Angel', ',', ...]\n",
      "['Ryan', 'hefted', 'his', 'bulk', 'up', 'and', ...]\n",
      "['She', 'lived', 'and', 'was', 'given', 'a', 'name', ...]\n",
      "['It', 'would', 'have', 'killed', 'you', 'in', 'the', ...]\n",
      "['Dan', 'Morgan', 'told', 'himself', 'he', 'would', ...]\n",
      "['Gavin', 'paused', 'wearily', '.', '``', 'You', ...]\n",
      "['The', 'sentry', 'was', 'not', 'dead', '.', 'He', ...]\n",
      "['``', 'So', 'it', \"wasn't\", 'the', 'earthquake', ...]\n",
      "['She', 'was', 'carrying', 'a', 'quirt', ',', 'and', ...]\n",
      "['Such', 'was', 'my', 'state', 'of', 'mind', 'that', ...]\n",
      "['The', 'flat', ',', 'hard', 'cap', 'was', 'small', ...]\n",
      "['If', 'she', 'sensed', 'any', 'unusual', ...]\n",
      "['Miraculously', ',', 'she', 'found', 'exactly', 'the', ...]\n",
      "['The', 'Brannon', 'outfit', '--', 'known', 'as', ...]\n",
      "['The', 'author', 'of', 'the', 'anonymous', 'notes', ...]\n",
      "['When', 'several', 'minutes', 'had', 'passed', 'and', ...]\n",
      "['Over', 'his', 'shoulder', 'he', 'could', 'see', ...]\n",
      "['While', 'no', 'larger', 'than', 'Dutch', 'Springs', ...]\n",
      "['Early', 'in', 'November', 'the', 'clouds', 'lifted', ...]\n",
      "['Over', 'the', 'rattling', 'of', 'fenders', ',', ...]\n",
      "['I', 'guided', 'her', 'to', 'the', 'divan', ',', ...]\n",
      "['Too', 'many', 'people', 'think', 'that', 'the', ...]\n",
      "['He', 'had', 'better', 'write', 'a', 'postcard', 'to', ...]\n",
      "['They', 'were', 'west', 'of', 'the', 'Sabine', ',', ...]\n",
      "['Sulphur', ',', 'oil', ',', 'and', 'copra', 'make', ...]\n",
      "['``', 'I', 'guess', 'he', 'spent', 'the', 'morning', ...]\n",
      "['Then', 'he', 'calmly', 'and', 'carefully', 'slugged', ...]\n",
      "['Now', ',', 'the', 'next', 'morning', ',', 'they', ...]\n",
      "['Early', 'that', 'day', 'Matsuo', 'saw', 'a', ...]\n",
      "['Chairs', 'scraped', 'back', 'and', 'customers', ...]\n",
      "['For', 'several', 'months', 'now', ',', 'Jack', ...]\n",
      "['On', 'the', 'fringe', 'of', 'the', 'amused', ...]\n",
      "['``', 'Bastards', \"''\", ',', 'he', 'would', 'say', ...]\n",
      "['They', 'neither', 'liked', 'nor', 'disliked', 'the', ...]\n",
      "['People', 'came', 'in', 'and', 'out', 'all', ...]\n",
      "['``', 'Thrifty', 'of', 'her', 'to', 'use', 'it', 'up', ...]\n",
      "['``', 'He', 'must', 'have', 'forgiven', 'me', \"''\", ...]\n",
      "['The', 'Momoyama', 'family', 'had', 'come', 'from', ...]\n",
      "['How', ',', 'he', 'wondered', ',', 'does', 'one', ...]\n",
      "['Spencer', 'said', 'nothing', '.', '``', 'Is', ...]\n",
      "['Rachel', 'steered', 'me', 'along', 'toward', 'a', ...]\n",
      "['``', 'And', \"I'll\", 'take', 'you', 'with', 'me', ...]\n",
      "['Among', 'us', ',', 'we', 'three', 'handled', 'quite', ...]\n",
      "['Such', 'a', 'little', 'thing', 'to', 'start', 'with', ...]\n",
      "['``', 'I', 'had', 'a', 'rather', 'small', 'place', ...]\n",
      "['But', 'one', 'night', 'Dookiyoon', 'moved', 'in', ...]\n",
      "['She', 'was', 'moving', 'through', 'a', 'screen', ...]\n",
      "['``', 'They', 'make', 'us', 'conformists', 'look', ...]\n",
      "['Unimpressed', ',', 'the', 'dog', 'plopped', 'on', ...]\n",
      "['When', 'Bobbie', 'Evans', 'smashed', 'up', 'his', ...]\n",
      "['She', 'called', 'then', 'to', 'say', 'she', 'had', ...]\n",
      "['I', \"don't\", 'really', 'believe', 'in', 'intuition', ...]\n",
      "['I', 'was', 'slowly', 'swimming', 'down', 'to', 'the', ...]\n",
      "['Two', 'letters', 'had', 'arrived', 'for', 'Miss', ...]\n",
      "['I', 'knew', 'it', 'as', 'surely', 'as', 'everybody', ...]\n",
      "['Under', 'normal', 'circumstances', ',', 'he', 'had', ...]\n",
      "['There', 'was', 'a', 'crowd', 'in', 'the', 'stands', ...]\n",
      "[\"Richard's\", 'next', 'interest', 'seemed', 'the', ...]\n",
      "['I', 'was', 'thinking', 'of', 'the', 'heat', 'and', ...]\n",
      "['Cady', \"didn't\", 'come', 'unglued', 'easily', ',', ...]\n",
      "['Martin', 'felt', 'it', 'was', 'incredible', 'that', ...]\n",
      "['``', 'Good', 'old', 'A-Z', \"''\", ',', 'Cap', 'said', ...]\n",
      "['It', 'was', 'among', 'these', 'that', 'Hinkle', ...]\n",
      "['I', 'realized', 'that', 'Hamlet', 'was', 'faced', ...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Needless', 'to', 'say', ',', 'I', 'was', 'furious', ...]\n",
      "['Up', 'to', 'date', ',', 'however', ',', 'his', ...]\n",
      "['Ambiguity', 'Nothing', 'in', 'English', 'has', ...]\n",
      "['I', 'called', 'the', 'other', 'afternoon', 'on', ...]\n",
      "['One', 'day', ',', 'the', 'children', 'had', 'wanted', ...]\n",
      "['Pueri', 'aquam', 'de', 'silvas', 'ad', 'agricolas', ...]\n",
      "['Dear', 'Sirs', ':', 'Let', 'me', 'begin', 'by', ...]\n",
      "  (0, 49)\t1.0\n",
      "  (0, 58)\t1.0\n",
      "  (0, 169)\t1.0\n",
      "  (0, 181)\t1.0\n",
      "  (0, 205)\t1.0\n",
      "  (0, 238)\t1.0\n",
      "  (0, 322)\t33.0\n",
      "  (0, 373)\t3.0\n",
      "  (0, 374)\t3.0\n",
      "  (0, 393)\t87.0\n",
      "  (0, 395)\t4.0\n",
      "  (0, 405)\t88.0\n",
      "  (0, 454)\t4.0\n",
      "  (0, 465)\t1.0\n",
      "  (0, 695)\t1.0\n",
      "  (0, 720)\t1.0\n",
      "  (0, 939)\t1.0\n",
      "  (0, 1087)\t1.0\n",
      "  (0, 1103)\t1.0\n",
      "  (0, 1123)\t1.0\n",
      "  (0, 1159)\t1.0\n",
      "  (0, 1170)\t1.0\n",
      "  (0, 1173)\t1.0\n",
      "  (0, 1200)\t3.0\n",
      "  (0, 1451)\t1.0\n",
      "  :\t:\n",
      "  (499, 49161)\t1.0\n",
      "  (499, 49164)\t1.0\n",
      "  (499, 49242)\t1.0\n",
      "  (499, 49253)\t1.0\n",
      "  (499, 49275)\t1.0\n",
      "  (499, 49301)\t1.0\n",
      "  (499, 49313)\t1.0\n",
      "  (499, 49369)\t1.0\n",
      "  (499, 49385)\t1.0\n",
      "  (499, 49386)\t4.0\n",
      "  (499, 49390)\t2.0\n",
      "  (499, 49410)\t2.0\n",
      "  (499, 49446)\t1.0\n",
      "  (499, 49576)\t1.0\n",
      "  (499, 49590)\t1.0\n",
      "  (499, 49613)\t3.0\n",
      "  (499, 49691)\t42.0\n",
      "  (499, 49694)\t3.0\n",
      "  (499, 49697)\t3.0\n",
      "  (499, 49698)\t1.0\n",
      "  (499, 49707)\t17.0\n",
      "  (499, 49708)\t1.0\n",
      "  (499, 49710)\t4.0\n",
      "  (499, 49711)\t1.0\n",
      "  (499, 49797)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word.lower()] = BOW.get(word.lower(),0) + 1\n",
    "    return BOW\n",
    "\n",
    "texts = []\n",
    "for fileid in brown.fileids():\n",
    "    print(brown.words(fileid))\n",
    "    texts.append(get_BOW(brown.words(fileid)))\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "brown_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "print(brown_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our matrix is sparse: for instance, columns 0-48 in row 0 are empty, and are just left out, only the rows and columns with values other than zeros are displayed\n",
    "\n",
    "Rather than removing stopwords as we did for text classification, let's add some idf weighting to this matrix. Scikit-learn has a built-in tf-idf transformer for just this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 49646)\t1.7298111649315369\n",
      "  (0, 49613)\t1.3681693233644676\n",
      "  (0, 49596)\t3.7066318654255337\n",
      "  (0, 49386)\t9.98833379406486\n",
      "  (0, 49378)\t8.731629015654066\n",
      "  (0, 49313)\t2.62964061975162\n",
      "  (0, 49301)\t7.374075931214787\n",
      "  (0, 49292)\t2.184170177029756\n",
      "  (0, 49224)\t3.385966701933097\n",
      "  (0, 49147)\t6.0\n",
      "  (0, 49041)\t3.407945608651872\n",
      "  (0, 49003)\t22.210096880912054\n",
      "  (0, 49001)\t5.741605353137016\n",
      "  (0, 48990)\t16.84677293625242\n",
      "  (0, 48951)\t4.7297014486341915\n",
      "  (0, 48950)\t4.939351940117883\n",
      "  (0, 48932)\t3.9565115604007097\n",
      "  (0, 48867)\t7.046120322868667\n",
      "  (0, 48777)\t1.41855034765682\n",
      "  (0, 48771)\t13.694210097452498\n",
      "  (0, 48769)\t6.236428984115791\n",
      "  (0, 48753)\t1.2957142441490452\n",
      "  (0, 48749)\t3.1984194075136347\n",
      "  (0, 48720)\t1.1648746431902341\n",
      "  (0, 48670)\t2.1974319458783156\n",
      "  :\t:\n",
      "  (499, 2710)\t3.120263536200091\n",
      "  (499, 2688)\t2.04412410338404\n",
      "  (499, 2670)\t3.9565115604007097\n",
      "  (499, 2611)\t4.270169119255751\n",
      "  (499, 2468)\t6.521460917862246\n",
      "  (499, 2439)\t4.170085660698769\n",
      "  (499, 2415)\t4.122633007848826\n",
      "  (499, 2413)\t2.320337504305643\n",
      "  (499, 2388)\t2.096614286005437\n",
      "  (499, 2358)\t6.115995809754082\n",
      "  (499, 2290)\t61.0\n",
      "  (499, 2289)\t7.5533024513831695\n",
      "  (499, 2286)\t11.156201344558639\n",
      "  (499, 2285)\t20.714812015222506\n",
      "  (499, 2283)\t1.2256466815323281\n",
      "  (499, 1345)\t6.521460917862246\n",
      "  (499, 1141)\t4.506557897319982\n",
      "  (499, 405)\t83.0\n",
      "  (499, 395)\t12.710333931244342\n",
      "  (499, 393)\t188.0\n",
      "  (499, 374)\t4.0872168559431525\n",
      "  (499, 373)\t4.095849955425997\n",
      "  (499, 354)\t7.214608098422191\n",
      "  (499, 322)\t7.538167310351703\n",
      "  (499, 320)\t3.4769384801388235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf=False,norm=None)\n",
    "\n",
    "brown_matrix_tfidf = transformer.fit_transform(brown_matrix)\n",
    "\n",
    "print(brown_matrix_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's apply SVD. Scikit-learn does not expose the internal details of the decomposition, we just use the [TruncatedSVD class](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) directly get a matrix with k dimensions. Since the Brown corpus is a fairly small corpus, we'll do k=10. Note that we'll first transpose the document-term sparse matrix to a term-document matrix before we apply SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49815, 10)\n",
      "[[ 1.46529922e+02 -1.56578509e+02  3.77240153e+01 ... -2.38690423e+01\n",
      "   7.41224830e+00 -3.69277068e+00]\n",
      " [ 6.10797743e-01  6.77340623e-01 -2.04566032e-01 ...  9.56899045e-01\n",
      "  -6.98441498e-02 -1.00807751e+00]\n",
      " [ 1.00411586e+00  1.99457219e-01 -1.24702678e-01 ... -1.08572551e+00\n",
      "  -4.44828384e-01 -3.49947544e-01]\n",
      " ...\n",
      " [ 3.26612758e-01  2.53370455e-01 -2.71162467e-01 ... -2.43841953e-01\n",
      "   1.06984529e-01  1.26377028e-01]\n",
      " [ 6.35382477e-01  7.12102156e-01 -2.80382620e-02 ... -6.73029289e-01\n",
      "   9.75824212e-02  3.28369748e-01]\n",
      " [ 3.27037764e-01  7.38771303e-01  2.09248449e+00 ...  2.21532794e-01\n",
      "  -3.47723331e-01  1.62275225e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "#dimension of brown_matrix_tfidf = num_documents x num_vocab\n",
    "#dimension of brown_matrix_tfidf_transposed = num_vocab x num_documents\n",
    "\n",
    "brown_matrix_tfidf_transposed = csr_matrix(brown_matrix_tfidf).transpose()\n",
    "\n",
    "svd = TruncatedSVD(n_components=10)\n",
    "brown_matrix_lowrank = svd.fit_transform(brown_matrix_tfidf_transposed)\n",
    "\n",
    "print(brown_matrix_lowrank.shape)\n",
    "print(brown_matrix_lowrank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned matrix corresponds to the transformed term/word matrix, $U \\Sigma$, after SVD factorisation, $X \\approx U \\Sigma V^T$, applied to `brown_matrix_tfidf_transposed`, as $X$. Note that the resulting matrix is not sparse.\n",
    "\n",
    "The last thing we'll do is to compare some words and see if their similarity fits our intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8421926002248518\n",
      "0.10815478470209312\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "v1 = brown_matrix_lowrank[vectorizer.vocabulary_[\"medical\"]]\n",
    "v2 = brown_matrix_lowrank[vectorizer.vocabulary_[\"health\"]]\n",
    "v3 = brown_matrix_lowrank[vectorizer.vocabulary_[\"gun\"]]\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    return np.dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "print(cos_sim(v1, v2))\n",
    "print(cos_sim(v1, v3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There'll be some variability to the exact cosine similarity values that you'll get (feel free to re-run SVD and check this), but hopefully you should find that _medical_ and _health_ is more closely related to each other than _medical_ and _gun_.\n",
    "\n",
    "Next let's try _information_, _retrieval_ and _science_!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5242523943488911\n",
      "0.3742219124413804\n"
     ]
    }
   ],
   "source": [
    "v1 = brown_matrix_lowrank[vectorizer.vocabulary_[\"information\"]]\n",
    "v2 = brown_matrix_lowrank[vectorizer.vocabulary_[\"retrieval\"]]\n",
    "v3 = brown_matrix_lowrank[vectorizer.vocabulary_[\"science\"]]\n",
    "\n",
    "print(cos_sim(v1, v2))\n",
    "print(cos_sim(v1, v3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did you find? Did you get similar results to Discussion Q2 in the worksheet? (you might want to re-run SVD and see if you find contradicting results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
