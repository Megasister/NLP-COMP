{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop, we will try to build some feedforward models to do sentiment analysis, using keras, a deep learning library: https://keras.io/\n",
    "\n",
    "You will need pandas, keras (2.3.1) and tensorflow (2.1.0; and their dependencies) to run this code (pip install pandas keras==2.3.1 tensorflow-cpu==2.1.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's prepare the data. We are using 1000 yelp reviews, nnotated with either positive or negative sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences = 1000\n",
      "\n",
      "Data:\n",
      "                                            sentence  label\n",
      "0                           Wow... Loved this place.      1\n",
      "1                                 Crust is not good.      0\n",
      "2          Not tasty and the texture was just nasty.      0\n",
      "3  Stopped by during the late May bank holiday of...      1\n",
      "4  The selection on the menu was great and so wer...      1\n",
      "5     Now I am getting angry and I want my damn pho.      0\n",
      "6              Honeslty it didn't taste THAT fresh.)      0\n",
      "7  The potatoes were like rubber and you could te...      0\n",
      "8                          The fries were great too.      1\n",
      "9                                     A great touch.      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "corpus = \"07-yelp-dataset.txt\"\n",
    "df = pd.read_csv(corpus, names=['sentence', 'label'], sep='\\t')\n",
    "print(\"Number of sentences =\", len(df))\n",
    "print(\"\\nData:\")\n",
    "print(df.iloc[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create the train/dev/test partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Wow... Loved this place.\n",
      "0 I'm super pissd.\n",
      "0 Spend your money elsewhere.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "sentences = df['sentence'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "#partition data into 80/10/10 for train/dev/test\n",
    "sentences_train, y_train = sentences[:800], labels[:800]\n",
    "sentences_dev, y_dev = sentences[800:900], labels[800:900]\n",
    "sentences_test, y_test = sentences[900:1000], labels[900:1000]\n",
    "\n",
    "#convert label list into arrays\n",
    "y_train = np.array(y_train)\n",
    "y_dev = np.array(y_dev)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(y_train[0], sentences_train[0])\n",
    "print(y_dev[0], sentences_dev[0])\n",
    "print(y_test[0], sentences_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize the text. In this workshop, we'll use the ``tokenizer`` function provided by keras. Once the data is tokenized, we can then use ``texts_to_matrix`` to get the bag-of-words representation for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 1811\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "x_train = tokenizer.texts_to_matrix(sentences_train, mode=\"count\") #BOW representation\n",
    "x_dev = tokenizer.texts_to_matrix(sentences_dev, mode=\"count\") #BOW representation\n",
    "x_test = tokenizer.texts_to_matrix(sentences_test, mode=\"count\") #BOW representation\n",
    "\n",
    "vocab_size = x_train.shape[1]\n",
    "print(\"Vocab size =\", vocab_size)\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we build a neural network model, let's see how well logistic regression do with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x_train, y_train)\n",
    "score = classifier.score(x_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression result is not too bad, and it will serve as a baseline for the deep learning models.\n",
    "\n",
    "Now let's build a very simple feedforward network. Here the input layer is the BOW features, and we have one hidden layer (dimension = 10) and an output layer in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/gey3/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"feedforward-bow-input\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                18120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 18,131\n",
      "Trainable params: 18,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "#model definition\n",
    "model = Sequential(name=\"feedforward-bow-input\")\n",
    "model.add(layers.Dense(10, input_dim=vocab_size, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "#since it's a binary classification problem, we use a binary cross entropy loss here\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model. Notice that there are a few hyper-parameters here, e.g. hidden layer size, number of epochs and batch_size, and in practice these hyper-parameters should be tuned according to the development data to get an optimal model. In this workshop we'll use 20 epochs and a batch size of 10 (no further tuning). Once the model is trained, we'll compute the test accuracy performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 100 samples\n",
      "Epoch 1/20\n",
      "800/800 [==============================] - 0s 305us/step - loss: 0.0252 - accuracy: 1.0000 - val_loss: 0.4330 - val_accuracy: 0.8400\n",
      "Epoch 2/20\n",
      "800/800 [==============================] - 0s 244us/step - loss: 0.0226 - accuracy: 1.0000 - val_loss: 0.4491 - val_accuracy: 0.8300\n",
      "Epoch 3/20\n",
      "800/800 [==============================] - 0s 225us/step - loss: 0.0204 - accuracy: 1.0000 - val_loss: 0.4540 - val_accuracy: 0.8400\n",
      "Epoch 4/20\n",
      "800/800 [==============================] - 0s 243us/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.4524 - val_accuracy: 0.8500\n",
      "Epoch 5/20\n",
      "800/800 [==============================] - 0s 224us/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.4619 - val_accuracy: 0.8400\n",
      "Epoch 6/20\n",
      "800/800 [==============================] - 0s 229us/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.4664 - val_accuracy: 0.8400\n",
      "Epoch 7/20\n",
      "800/800 [==============================] - 0s 238us/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.4708 - val_accuracy: 0.8500\n",
      "Epoch 8/20\n",
      "800/800 [==============================] - 0s 221us/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.4801 - val_accuracy: 0.8500\n",
      "Epoch 9/20\n",
      "800/800 [==============================] - 0s 226us/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.4870 - val_accuracy: 0.8500\n",
      "Epoch 10/20\n",
      "800/800 [==============================] - 0s 228us/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.4983 - val_accuracy: 0.8400\n",
      "Epoch 11/20\n",
      "800/800 [==============================] - 0s 223us/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.4996 - val_accuracy: 0.8400\n",
      "Epoch 12/20\n",
      "800/800 [==============================] - 0s 225us/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.5064 - val_accuracy: 0.8300\n",
      "Epoch 13/20\n",
      "800/800 [==============================] - 0s 216us/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.5161 - val_accuracy: 0.8300\n",
      "Epoch 14/20\n",
      "800/800 [==============================] - 0s 230us/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.5178 - val_accuracy: 0.8300\n",
      "Epoch 15/20\n",
      "800/800 [==============================] - 0s 226us/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.5210 - val_accuracy: 0.8200\n",
      "Epoch 16/20\n",
      "800/800 [==============================] - 0s 224us/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.5321 - val_accuracy: 0.8300\n",
      "Epoch 17/20\n",
      "800/800 [==============================] - 0s 229us/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.5377 - val_accuracy: 0.8300\n",
      "Epoch 18/20\n",
      "800/800 [==============================] - 0s 226us/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.5426 - val_accuracy: 0.8300\n",
      "Epoch 19/20\n",
      "800/800 [==============================] - 0s 225us/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.5494 - val_accuracy: 0.8300\n",
      "Epoch 20/20\n",
      "800/800 [==============================] - 0s 226us/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.5548 - val_accuracy: 0.8300\n",
      "\n",
      "Testing Accuracy:  0.7700\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "model.fit(x_train, y_train, epochs=20, verbose=True, validation_data=(x_dev, y_dev), batch_size=10)\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
    "print(\"\\nTesting Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the performance compare to logistic regression? If you run it a few times you may find that it gives slightly different numbers, and that is due to random initialisation of the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we did not explicitly define any word embeddings in the model architecture, they are in our model: in the weights between the input and the hidden layer. The hidden layer can therefore be interpreted as a sum of word embeddings for each input document.\n",
    "\n",
    "Let's fetch the word embeddings of some words, and look at their cosine similarity, and see if they make any sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.24366257 -0.42550236  0.28396595  0.24380517 -0.25342706  0.2706588\n",
      "  0.32807893  0.34532306 -0.36072746  0.37499902]\n",
      "love vs. like = 0.9268172\n",
      "love vs. lukewarm = -0.9849436\n",
      "love vs. bad = -0.9830696\n",
      "lukewarm vs. bad = 0.991907\n"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "embeddings = model.get_layer(index=0).get_weights()[0] #word embeddings layer\n",
    "\n",
    "emb_love = embeddings[tokenizer.word_index[\"love\"]] #embeddings for 'love'\n",
    "emb_like = embeddings[tokenizer.word_index[\"like\"]]\n",
    "emb_lukewarm = embeddings[tokenizer.word_index[\"lukewarm\"]]\n",
    "emb_bad = embeddings[tokenizer.word_index[\"bad\"]]\n",
    "\n",
    "print(emb_love)\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "print(\"love vs. like =\", cos_sim(emb_love, emb_like))\n",
    "print(\"love vs. lukewarm =\", cos_sim(emb_love, emb_lukewarm))\n",
    "print(\"love vs. bad =\", cos_sim(emb_love, emb_bad))\n",
    "print(\"lukewarm vs. bad =\", cos_sim(emb_lukewarm, emb_bad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. You should find that for *love* and *like*, which are both positive sentiment words, produce high cosine similarity. Similar observations for *lukewarm* and *bad*. But when we compare opposite polarity words like *love* and *bad*, we get negative cosine similarity values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to build another feed-forward model, but this time, instead of using BOW features as input, we want to use the word sequence as input (so order of words is preserved). It is usually not straightforward to do this for classical machine learning models, but with neural networks and embeddings, it's pretty straightforward.\n",
    "\n",
    "Let's first tokenise the input documents into word sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[354, 138, 9, 17]\n"
     ]
    }
   ],
   "source": [
    "#tokenise the input into word sequences\n",
    "\n",
    "xseq_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "xseq_dev = tokenizer.texts_to_sequences(sentences_dev)\n",
    "xseq_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "print(xseq_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because documents have variable lengths, we need to first 'pad' them to make all documents have the same length. keras uses word index 0 to represent 'pad symbols'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[354 138   9  17   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 30\n",
    "xseq_train = pad_sequences(xseq_train, padding='post', maxlen=maxlen)\n",
    "xseq_dev = pad_sequences(xseq_dev, padding='post', maxlen=maxlen)\n",
    "xseq_test = pad_sequences(xseq_test, padding='post', maxlen=maxlen)\n",
    "print(xseq_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build our second model. This model first embeds each word in the input sequence into embeddings, and then concatenate the word embeddings together to represent input sequence. The ``Flatten`` function you see after the embedding layer is essentially doing the concatenation, by 'chaining' the list of word embeddings into a very long vector.\n",
    "\n",
    "If our word embeddings has a dimension 10, and our documents always have 30 words (padded), then here the concatenated word embeddings have a dimension of 10 x 30 = 300. \n",
    "\n",
    "The concatenated word embeddings undergo a linear transformation with non-linear activations (``layers.Dense(10, activation='relu')``), producing a hidden representation with a dimension of 10. It is then passed to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"feedforward-sequence-input\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 10)            18110     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                3010      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 21,131\n",
      "Trainable params: 21,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 10\n",
    "\n",
    "#word order preserved with this architecture\n",
    "model2 = Sequential(name=\"feedforward-sequence-input\")\n",
    "model2.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(10, activation='relu'))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model and compute the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 100 samples\n",
      "Epoch 1/20\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.6861 - accuracy: 0.5525 - val_loss: 0.7134 - val_accuracy: 0.4400\n",
      "Epoch 2/20\n",
      "800/800 [==============================] - 0s 254us/step - loss: 0.6616 - accuracy: 0.5987 - val_loss: 0.6942 - val_accuracy: 0.4800\n",
      "Epoch 3/20\n",
      "800/800 [==============================] - 0s 277us/step - loss: 0.5904 - accuracy: 0.7175 - val_loss: 0.6449 - val_accuracy: 0.5700\n",
      "Epoch 4/20\n",
      "800/800 [==============================] - 0s 246us/step - loss: 0.4282 - accuracy: 0.8950 - val_loss: 0.5305 - val_accuracy: 0.7600\n",
      "Epoch 5/20\n",
      "800/800 [==============================] - 0s 277us/step - loss: 0.2447 - accuracy: 0.9625 - val_loss: 0.4744 - val_accuracy: 0.7600\n",
      "Epoch 6/20\n",
      "800/800 [==============================] - 0s 269us/step - loss: 0.1275 - accuracy: 0.9850 - val_loss: 0.4498 - val_accuracy: 0.7900\n",
      "Epoch 7/20\n",
      "800/800 [==============================] - 0s 269us/step - loss: 0.0727 - accuracy: 0.9937 - val_loss: 0.4455 - val_accuracy: 0.8000\n",
      "Epoch 8/20\n",
      "800/800 [==============================] - 0s 261us/step - loss: 0.0451 - accuracy: 0.9962 - val_loss: 0.4608 - val_accuracy: 0.7800\n",
      "Epoch 9/20\n",
      "800/800 [==============================] - 0s 250us/step - loss: 0.0288 - accuracy: 1.0000 - val_loss: 0.4454 - val_accuracy: 0.7900\n",
      "Epoch 10/20\n",
      "800/800 [==============================] - 0s 247us/step - loss: 0.0197 - accuracy: 1.0000 - val_loss: 0.4619 - val_accuracy: 0.7800\n",
      "Epoch 11/20\n",
      "800/800 [==============================] - 0s 248us/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.4686 - val_accuracy: 0.7800\n",
      "Epoch 12/20\n",
      "800/800 [==============================] - 0s 266us/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.4865 - val_accuracy: 0.7800\n",
      "Epoch 13/20\n",
      "800/800 [==============================] - 0s 267us/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.4950 - val_accuracy: 0.7500\n",
      "Epoch 14/20\n",
      "800/800 [==============================] - 0s 289us/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.4863 - val_accuracy: 0.7800\n",
      "Epoch 15/20\n",
      "800/800 [==============================] - 0s 259us/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.4850 - val_accuracy: 0.7900\n",
      "Epoch 16/20\n",
      "800/800 [==============================] - 0s 281us/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.4970 - val_accuracy: 0.7800\n",
      "Epoch 17/20\n",
      "800/800 [==============================] - 0s 271us/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5147 - val_accuracy: 0.7800\n",
      "Epoch 18/20\n",
      "800/800 [==============================] - 0s 276us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5033 - val_accuracy: 0.7900\n",
      "Epoch 19/20\n",
      "800/800 [==============================] - 0s 263us/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5078 - val_accuracy: 0.8000\n",
      "Epoch 20/20\n",
      "800/800 [==============================] - 0s 285us/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.5266 - val_accuracy: 0.8000\n",
      "Testing Accuracy:  0.6900\n"
     ]
    }
   ],
   "source": [
    "model2.fit(xseq_train, y_train, epochs=20, verbose=True, validation_data=(xseq_dev, y_dev), batch_size=10)\n",
    "\n",
    "loss, accuracy = model2.evaluate(xseq_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find that the performance isn't as good as the BOW model. In general, concatenating word embeddings isn't a good way to represent word sequence.\n",
    "\n",
    "A better way is to build a recurrent model. But first, let's extract the word embeddings for the 4 words as before and look at their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love vs. like = 0.16650452\n",
      "love vs. lukewarm = -0.9086337\n",
      "love vs. bad = -0.7283029\n",
      "lukewarm vs. bad = 0.7826571\n"
     ]
    }
   ],
   "source": [
    "embeddings = model2.get_layer(index=0).get_weights()[0] #word embeddings\n",
    "\n",
    "emb_love = embeddings[tokenizer.word_index[\"love\"]]\n",
    "emb_like = embeddings[tokenizer.word_index[\"like\"]]\n",
    "emb_lukewarm = embeddings[tokenizer.word_index[\"lukewarm\"]]\n",
    "emb_bad = embeddings[tokenizer.word_index[\"bad\"]]\n",
    "\n",
    "print(\"love vs. like =\", cos_sim(emb_love, emb_like))\n",
    "print(\"love vs. lukewarm =\", cos_sim(emb_love, emb_lukewarm))\n",
    "print(\"love vs. bad =\", cos_sim(emb_love, emb_bad))\n",
    "print(\"lukewarm vs. bad =\", cos_sim(emb_lukewarm, emb_bad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to build an LSTM model. After the embeddings layer, the LSTM layer will process the words one at a time, and compute the next state (dimension for the hidden state = 10 in this case). The output of the LSTM layer is the final state, produced after processing the last word, and that will be fed to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstm\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 30, 10)            18110     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 18,961\n",
      "Trainable params: 18,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "#word order preserved with this architecture\n",
    "model3 = Sequential(name=\"lstm\")\n",
    "model3.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model3.add(LSTM(10))\n",
    "model3.add(layers.Dense(1, activation='sigmoid'))\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the LSTM model and see the test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 100 samples\n",
      "Epoch 1/20\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.6890 - accuracy: 0.5575 - val_loss: 0.7097 - val_accuracy: 0.4400\n",
      "Epoch 2/20\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.6827 - accuracy: 0.5650 - val_loss: 0.7081 - val_accuracy: 0.4400\n",
      "Epoch 3/20\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.6196 - accuracy: 0.6488 - val_loss: 0.6282 - val_accuracy: 0.7400\n",
      "Epoch 4/20\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.3969 - accuracy: 0.8950 - val_loss: 0.5049 - val_accuracy: 0.8000\n",
      "Epoch 5/20\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.2885 - accuracy: 0.9187 - val_loss: 0.5267 - val_accuracy: 0.8200\n",
      "Epoch 6/20\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.2032 - accuracy: 0.9500 - val_loss: 0.6291 - val_accuracy: 0.7900\n",
      "Epoch 7/20\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.2046 - accuracy: 0.9425 - val_loss: 0.5844 - val_accuracy: 0.7900\n",
      "Epoch 8/20\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.1442 - accuracy: 0.9675 - val_loss: 0.5725 - val_accuracy: 0.8200\n",
      "Epoch 9/20\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.1111 - accuracy: 0.9775 - val_loss: 0.7267 - val_accuracy: 0.7800\n",
      "Epoch 10/20\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.0831 - accuracy: 0.9850 - val_loss: 0.7070 - val_accuracy: 0.7900\n",
      "Epoch 11/20\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.0661 - accuracy: 0.9862 - val_loss: 0.7488 - val_accuracy: 0.7900\n",
      "Epoch 12/20\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.0488 - accuracy: 0.9925 - val_loss: 0.6754 - val_accuracy: 0.8000\n",
      "Epoch 13/20\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.0289 - accuracy: 0.9975 - val_loss: 0.7795 - val_accuracy: 0.8000\n",
      "Epoch 14/20\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.0251 - accuracy: 0.9975 - val_loss: 0.8253 - val_accuracy: 0.8000\n",
      "Epoch 15/20\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.0229 - accuracy: 0.9975 - val_loss: 0.8672 - val_accuracy: 0.7800\n",
      "Epoch 16/20\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.0211 - accuracy: 0.9975 - val_loss: 0.9413 - val_accuracy: 0.7700\n",
      "Epoch 17/20\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.0151 - accuracy: 0.9975 - val_loss: 0.9901 - val_accuracy: 0.7800\n",
      "Epoch 18/20\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.0259 - accuracy: 0.9950 - val_loss: 1.0487 - val_accuracy: 0.7900\n",
      "Epoch 19/20\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.0206 - accuracy: 0.9975 - val_loss: 1.0212 - val_accuracy: 0.7900\n",
      "Epoch 20/20\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.0134 - accuracy: 0.9987 - val_loss: 1.0205 - val_accuracy: 0.8000\n",
      "Testing Accuracy:  0.7100\n"
     ]
    }
   ],
   "source": [
    "model3.fit(xseq_train, y_train, epochs=20, verbose=True, validation_data=(xseq_dev, y_dev), batch_size=10)\n",
    "\n",
    "loss, accuracy = model3.evaluate(xseq_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that the training is quite a bit slower, and that's because now the model has to process the sequence one word at a time. But the results should be better!\n",
    "\n",
    "And lastly, let's extract the embeddings and look at the their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love vs. like = 0.9107263\n",
      "love vs. lukewarm = -0.9386679\n",
      "love vs. bad = -0.8816992\n",
      "lukewarm vs. bad = 0.80625194\n"
     ]
    }
   ],
   "source": [
    "embeddings = model3.get_layer(index=0).get_weights()[0] #word embeddings\n",
    "\n",
    "emb_love = embeddings[tokenizer.word_index[\"love\"]]\n",
    "emb_like = embeddings[tokenizer.word_index[\"like\"]]\n",
    "emb_lukewarm = embeddings[tokenizer.word_index[\"lukewarm\"]]\n",
    "emb_bad = embeddings[tokenizer.word_index[\"bad\"]]\n",
    "\n",
    "print(\"love vs. like =\", cos_sim(emb_love, emb_like))\n",
    "print(\"love vs. lukewarm =\", cos_sim(emb_love, emb_lukewarm))\n",
    "print(\"love vs. bad =\", cos_sim(emb_love, emb_bad))\n",
    "print(\"lukewarm vs. bad =\", cos_sim(emb_lukewarm, emb_bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
